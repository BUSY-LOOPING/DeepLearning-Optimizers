# DeepLearning-Optimizers
Visualization of popular Deep Learning optimizers built upon Gradient Descent.

## Cost Function
$z = x^2 - y^2$ , or, $J = \theta_1^2 - \theta_2^2$

<img src="saved/pic2.png" height = "250">

### Vanilla Gradient Descent
<img src="saved/vanilla_gd_3d.svg" height = "250">

### Momentum
<img src="saved/momentum_3d.svg" height = "250">

### AdaGrad
<img src="saved/adagrad_3d.svg" height = "250">

### AdaDelta
<img src="saved/adadelta._3d.svg" height = "250">

## Costs Comparison
<img src="saved/costs_compare.svg" height = "250">